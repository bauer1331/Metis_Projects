{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "[Import](#Import)  \n",
    "[Functions](#Functions)  \n",
    "[Data Processing](#Data-Processing)  \n",
    "[Fix Strings](#Fix-Strings)  \n",
    "[Dummy Variables](#Dummy-Variables)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import  \n",
    "[Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from textblob import TextBlob\n",
    "\n",
    "import re\n",
    "import copy\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bauer/ds/metis/metisgh/Metis_Projects/Community_Survey/Code\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../Data_Prep/AVENCensus2016_data.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4161a4b2813e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../Data_Prep/AVENCensus2016_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# df=df.drop(['Unnamed: 0'],1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bauer/anaconda3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    644\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bauer/anaconda3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bauer/anaconda3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bauer/anaconda3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    921\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bauer/anaconda3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:4184)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:8449)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'../Data_Prep/AVENCensus2016_data.csv' does not exist"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('Users/bauer/Ace/acegh//Data_Prep/AVENCensus2016_data.csv')\n",
    "# df=df.drop(['Unnamed: 0'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9850, 310)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"../Data_Prep/2016_likertDict.pkl\", 'rb') as picklefile: \n",
    "    likertDict = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# list(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "[Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_words(model, feature_names, n_top_words):\n",
    "    lst = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "#         lst.append(topic_idx)\n",
    "        lst.append(\" , \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nmfTopics(data, n_topics, n_top_words, n_features):\n",
    "\n",
    "    # Use tf-idf features for NMF.\n",
    "    print(\"Extracting tf-idf features for NMF...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df=4, max_df=.25, \n",
    "#                                        max_features=int(len(data)),\n",
    "                                       stop_words='english', ngram_range=(3, 3))\n",
    "    t0 = time()\n",
    "    tfidf = tfidf_vectorizer.fit_transform(data)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "    # Fit the NMF model\n",
    "    print(\"Fitting the NMF model with tf-idf features, \"\n",
    "          \"n_samples=%d and n_features=%d...\"\n",
    "          % (int(len(data)), int(len(data)/2)))\n",
    "    t0 = time()\n",
    "    nmf = NMF(n_components=n_topics, random_state=1,\n",
    "              alpha=.1, l1_ratio=.3).fit(tfidf)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "    print(\"\\nTopics in NMF model:\")\n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    return top_words(nmf, tfidf_feature_names, n_top_words), nmf.fit_transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def processing(data, col_num):\n",
    "#     data = data.dropna(subset=[data.columns[col_num]])\n",
    "    data = data.dropna(subset=[col_num])\n",
    "    data = data.ix[:,col_num]\n",
    "    data2 = data\n",
    "    data.tolist()\n",
    "    data = [x.lower() for x in data]\n",
    "#     data2 = []\n",
    "#     for i in range(len(data)):  \n",
    "#         temp = []\n",
    "#         for word in TextBlob(data[i]).words:\n",
    "#             werd = stemmer.stem(word)\n",
    "#             #print(' '.join(werd))\n",
    "#             #print(werd)\n",
    "#             temp.append(werd)\n",
    "#             #\" \".join(temp)\n",
    "#         data2.append(temp)\n",
    "#         data2[i] = \" \".join(data2[i])\n",
    "    return data, data2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "[Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "long_answers = [\"feedback\", \"outResponceLong\", \"discrimLong\", \"aceSupportLong\",  \"allySupportLong\",\"questions\", \"stories\" ]\n",
    "for i in long_answers:\n",
    "    df[i].replace([np.nan],[\";\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['test'] =  df[\"outResponceLong\"].map(str)+\" \"+  df[\"discrimLong\"].map(str)+\" \"+  df[\"aceSupportLong\"].map(str)\\\n",
    "            +\" \"+  df[\"allySupportLong\"].map(str)  + \" \"+ df[\"questions\"].map(str)+ \" \"+ df[\"stories\"].map(str)#+ df[\"feedback\"].map(str) +\" \"+\n",
    "# df[df['test'].notnull()]\n",
    "df['test'].replace(['; ; ; ; ; ;'],[np.nan], inplace = True)\n",
    "df['test'].replace([';'],[np.nan], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['support_long'] = df[\"aceSupportLong\"].map(str)+\" \"+  df[\"allySupportLong\"].map(str)\n",
    "df['support_long'].replace(['   '],[np.nan], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['outResponceLong'].replace([';'],[np.nan],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "discrim, discrim_df = processing(df, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6311"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(discrim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# discrim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# for i in range(len(discrim)):\n",
    "#     tagged_sent = pos_tag(discrim[i].split())\n",
    "#     adjs = [word for word,pos in tagged_sent if pos == 'JJ']\n",
    "#     for j in range(len(adjs)):\n",
    "#         print('\\n',adjs[j], '\\n', TextBlob(adjs[j]).sentiment )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adjs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e4b1018db10f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0madjs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'adjs' is not defined"
     ]
    }
   ],
   "source": [
    "adjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=2, max_df=.5, \n",
    "#                                   max_features=int(len(data)),\n",
    "                                    stop_words='english',strip_accents = 'unicode', lowercase = True, ngram_range=(3, 3))\n",
    "tfidf = tfidf_vectorizer.fit_transform(discrim)\n",
    "nmf = NMF(n_components=5, random_state=1,\n",
    "              alpha=.1, l1_ratio=.3).fit(tfidf)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                               stop_words = 'english',\n",
    "                               lowercase = True,\n",
    "                               token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                               max_df = 0.5,\n",
    "                               min_df = 2,\n",
    "                               ngram_range = (3,3))\n",
    "dtm_tf = tf_vectorizer.fit_transform(discrim)\n",
    "lda_tf = LatentDirichletAllocation(n_topics=10, random_state=0, n_jobs = -1)\n",
    "lda_tf.fit(dtm_tf)\n",
    "data = pyLDAvis.sklearn.prepare(lda_tf, dtm_tf, tf_vectorizer)\n",
    "pyLDAvis.display(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.sklearn\n",
    "tf_vectorizer = CountVectorizer(strip_accents = 'unicode',\n",
    "                               stop_words = 'english',\n",
    "                               lowercase = True,\n",
    "                               token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                               max_df = 0.5,\n",
    "                               min_df = 10)\n",
    "dtm_tf = tf_vectorizer.fit_transform(discrim)\n",
    "lda_tf = LatentDirichletAllocation(n_topics=10, random_state=0, n_jobs = -1)\n",
    "lda_tf.fit(dtm_tf)\n",
    "data = pyLDAvis.sklearn.prepare(lda_tf, dtm_tf, tf_vectorizer)\n",
    "pyLDAvis.display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lda_tf.fit(dtm_tf)\n",
    "tfidf_vectorizer.fit(discrim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import prepare from sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data = pyLDAvis.sklearn.prepare(lda_tf, dtm_tf, tf_vectorizer)\n",
    "data = pyLDAvis.sklearn.prepare(nmf, discrim, tfidf_vectorizer)\n",
    "pyLDAvis.display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenize everything \n",
    "train['tokenizer'] = train['col'].apply(lambda x: word_tokenize(x))\n",
    "countvetor(strip_accents = 'unicode', stop_word ='english', lowercase = True, max_df, min_df)\n",
    "\n",
    "\n",
    "get shape of fit_transform\n",
    "\n",
    "data = dyLDAvis.sklearn.prepare(model, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topWords = top_words(nmf, tfidf_feature_names, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = (\"The sun is shiny i like the sun\",\"I have been exposed to sun\")\n",
    "vect = TfidfVectorizer(stop_words=\"english\",lowercase=False)\n",
    "mat = vect.fit_transform(sentences).toarray()\n",
    "print(type(mat))\n",
    "print(vect.vocabulary_)\n",
    "words = ['']*len(vect.vocabulary_)\n",
    "for k,v in vect.vocabulary_.items():\n",
    "    words[v] = k\n",
    "print(words)\n",
    "print(vect.idf_)\n",
    "# q = mat / vect.idf_\n",
    "# sums = np.ones((q.shape[0], 1))\n",
    "# lens = np.ones((q.shape[0], 1))\n",
    "# for ix in range(q.shape[0]):\n",
    "#     sums[ix] = np.sum(q[ix,:])\n",
    "#     lens[ix] = len([x for x in sentences[ix].split() if x in vect.get_feature_names()]) #have to filter out stopwords\n",
    "# sum_to_1 = q / sums\n",
    "# tf = sum_to_1 * lens\n",
    "# print(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters = [list(r).index(max(r)) for r in nmf.fit_transform(tfidf)]\n",
    "clusters_Series = pd.DataFrame(clusters, index =discrim_df.index )\n",
    "df_clus = df.join(clusters_Series)\n",
    "# discrim_topic_df = df_clus\n",
    "df_clus[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sentences = (\"The sun is shiny i like the sun\",\"I have been exposed to sun\")\n",
    "# vect = TfidfVectorizer(stop_words=\"english\",lowercase=False)\n",
    "# mat = vect.fit_transform(sentences).toarray()\n",
    "mat = tfidf_vectorizer.fit_transform(discrim)#.toarray()\n",
    "print(mat.shape)\n",
    "print(tfidf_vectorizer.get_feature_names().shape)\n",
    "q = mat / tfidf_vectorizer.idf_\n",
    "sums = np.ones((q.shape[0], 1))\n",
    "lens = np.ones((q.shape[0], 1))\n",
    "for ix in range(q.shape[0]):\n",
    "    sums[ix] = np.sum(q[ix,:])\n",
    "    lens[ix] = len([x for x in discrim[ix].split() if x in topWords]) #have to filter out stopwords\n",
    "sum_to_1 = q / sums\n",
    "tf = sum_to_1 * lens\n",
    "# print(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=4, max_df=.5, \n",
    "#                                   max_features=int(len(data)),\n",
    "                                    stop_words='english', ngram_range=(3, 3))\n",
    "tfidf = tfidf_vectorizer.fit_transform(discrim)\n",
    "nmf = NMF(n_components=5, random_state=1,\n",
    "              alpha=.1, l1_ratio=.3).fit(tfidf)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "top_words(nmf, tfidf_feature_names, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters = [list(r).index(max(r)) for r in nmf.fit_transform(tfidf)]\n",
    "clusters_Series = pd.DataFrame(clusters, index =discrim_df.index )\n",
    "df_clus = df.join(clusters_Series)\n",
    "# discrim_topic_df = df_clus\n",
    "df_clus[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "discrim_topics, discrim_transform = nmfTopics(discrim,5,5,500)\n",
    "discrim_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusters = [list(r).index(max(r)) for r in discrim_transform]\n",
    "clusters_Series = pd.DataFrame(clusters, index =discrim_df.index )\n",
    "df_clus = df.join(clusters_Series)\n",
    "# discrim_topic_df = df_clus[df_clus['discrimLong'].notnull()]\n",
    "discrim_topic_df = df_clus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_clus[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "discrim_topics, discrim_transform = nmfTopics(discrim, 10,10,500)\n",
    "discrim_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# discrim_transform[1000:1025]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_clus[df_clus[0]==0]['outResponceLong']#.value_counts()\n",
    "df_clus[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(discrim_topics)):\n",
    "    print('\\n',discrim_topics[i], '\\n', TextBlob(discrim_topics[i]).sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(discrim_topics)):\n",
    "    words = discrim_topics[i].split()\n",
    "    for j in range(len(words)):\n",
    "        print('\\n',words[j], '\\n', TextBlob(words[j]).sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "discrim_topic_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "discrim_topic_df['trans'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "discrim_topic_df[discrim_topic_df['trans']=='Yes'][0].value_counts()/len(discrim_topic_df[discrim_topic_df['trans']=='Yes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "discrim_topic_df[discrim_topic_df['trans']=='No'][0].value_counts()/len(discrim_topic_df[discrim_topic_df['trans']=='No'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "discrim_topic_df[discrim_topic_df['trans']=='Unsure'][0].value_counts()/len(discrim_topic_df[discrim_topic_df['trans']=='Unsure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "discrim_topic_df[discrim_topic_df['gender']=='None of the above'][0].value_counts()/ len(discrim_topic_df[discrim_topic_df['gender']=='None of the above'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "discrim_topic_df[discrim_topic_df['gender']=='Woman/female'][0].value_counts()/ len(discrim_topic_df[discrim_topic_df['gender']=='Woman/female'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "discrim_topic_df[discrim_topic_df['gender']=='Man/male'][0].value_counts()/ len(discrim_topic_df[discrim_topic_df['gender']=='Man/male'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# discrim_topic_df[discrim_topic_df['year']<1996]['alcohol'].value_counts()#/len(discrim_topic_df[discrim_topic_df['year']<1996]['alcohol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "discrim_topic_df[discrim_topic_df['alcohol']<1][0].value_counts()#/ len(discrim_topic_df[discrim_topic_df['alcohol']<1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "discrim_topic_df[discrim_topic_df['alcohol']>1][0].value_counts()#/ len(discrim_topic_df[discrim_topic_df['alcohol']>1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "topic_words = []\n",
    "for r in model.components_:\n",
    "    a = sorted([(v,i) for i,v in enumerate(r)],reverse=True)[0:7]\n",
    "    topic_words.append([words[e[1]] for e in a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix Strings\n",
    "[Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stringing = 'hi (my, name, is) lots, of commas, here'\n",
    "re.sub(r'\\([^)]*\\)', '', stringing)\n",
    "#stringing.replace(r'([^0-9]', '')\n",
    "#a = re.search(r'Release Date:</h4>[ ]+[0-9,a-z A-Z]+', page)\n",
    "#b = re.search(r'>[ ][0-9,a-z A-Z]+', a.group(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Variables\n",
    "[Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#list(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df.ix[2,'Do you identify with any of the following racial/ethnic categories?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "race = df['Do you identify with any of the following racial/ethnic categories?']\n",
    "race = race.str.replace(r'\\([^)]*\\)', '')\n",
    "race=race.str.replace(r'[ ]', '')\n",
    "race = pd.Series([str.lower(str(i)).strip() for i in race])\n",
    "\n",
    "race = race.str.get_dummies(sep=',') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "race.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "race_cols = pd.DataFrame.sum(race)\n",
    "fiveup = [race_cols.index[i] for i in range(len(race_cols.index)) if race_cols[i]>5.0]\n",
    "fiveup = race[fiveup]\n",
    "for i in fiveup:\n",
    "    fiveup[i].fillna(0, inplace=True)\n",
    "fiveup.columns = ['aboriginalAustralian','asianEastern','asianSoutheast','asianSouthern','asianWestern',\n",
    "                  'blackAfrican','blackCaribbean','blackDiaspora', 'brown','hispanic','jewish','latinx','middleEastern',\n",
    "                 'mixedRace','raceNoneSpecified','raceNo','northAfrican','northAmericanNative','pacificIslanderPoly',\n",
    "                  'southCentAmericanNative','whiteOrEuropean']\n",
    "fiveup.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "fiveup_cols = pd.DataFrame.sum(fiveup)\n",
    "fiveup_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
